{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcecbc2-bff8-4a7e-9401-27a466664ecc",
   "metadata": {},
   "source": [
    "# Role of Tokenizers\n",
    "---\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) such as words, subwords, or characters. Tokenization is a crucial step in preparing text data for models. Different algorithms like BPE, WordPiece, and Unigram have their own strategies for splitting and encoding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ccde9b-5b58-46f6-8819-c91b1f543393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "training_data = [\n",
    "    \"walker walked a long walk\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91352d8-191e-4f1b-ba08-a4d76d67ce5a",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE)\n",
    "\n",
    "BPE is a subword tokenization algorithm that iteratively merges the most frequent character pairs. It reduces the number of unknown tokens while keeping the vocabulary size manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63b06d7-5880-4518-8c6f-0d4329fcc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Byte Pain Encoding - BPE\n",
    "\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "bpe_tokenizer = Tokenizer(BPE())\n",
    "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "bpe_trainer = BpeTrainer(vocab_size=14)\n",
    "\n",
    "bpe_tokenizer.train_from_iterator(training_data, bpe_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8401e94-69dc-42fd-b963-3b91c3f922f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 8,\n",
       " 'k': 4,\n",
       " 'wal': 11,\n",
       " 'walk': 12,\n",
       " 'w': 9,\n",
       " 'walke': 13,\n",
       " 'al': 10,\n",
       " 'n': 6,\n",
       " 'l': 5,\n",
       " 'd': 1,\n",
       " 'o': 7,\n",
       " 'a': 0,\n",
       " 'e': 2,\n",
       " 'g': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display the learned vocabulary\n",
    "\"\"\"\n",
    "bpe_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95cedd5-dc21-4dbb-bbe9-9459e2117a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walke', 'r', 'walke', 'd', 'a', 'l', 'o', 'n', 'g', 'walk']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell demonstrates how the BPE tokenizer splits and encodes text based on the learned vocabulary.\n",
    "\"\"\"\n",
    "bpe_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfba9115-4bd6-48ad-a8b5-2ac15f0f70d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 5, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"wlk\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999503e7-f237-46f2-b818-88cc7b318e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'l', 'k']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a46c10-f2c8-4783-aa10-5cca14df8f20",
   "metadata": {},
   "source": [
    "## Unknown Tokens in BPE\n",
    "\n",
    "BPE does not rely on a fixed dictionary of words, allowing it to split unknown tokens into smaller subword units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb5ac7b-b641-445b-aebe-076ed7e79c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'walke', 'd']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e57a92-c07b-47c6-b769-0abf25d25e7d",
   "metadata": {},
   "source": [
    "## WordPiece Tokenization\n",
    "\n",
    "WordPiece is similar to BPE but focuses on maximizing the likelihood of the training data. It is widely used in models like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48339523-9976-41b8-a2f7-969ce38b522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WordPiece\n",
    "\n",
    "from real_wordpiece.trainer import RealWordPieceTrainer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "real_wordpiece_tokenizer = Tokenizer(WordPiece())\n",
    "real_wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "real_wordpiece_trainer = RealWordPieceTrainer(\n",
    "    vocab_size=27,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d1e22ae-2a6d-436a-94cf-636c705e54da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wa': 24,\n",
       " '##g': 11,\n",
       " 'w': 0,\n",
       " 'g': 18,\n",
       " '##ng': 20,\n",
       " '##k': 3,\n",
       " 'o': 16,\n",
       " 'e': 13,\n",
       " 'walk': 26,\n",
       " 'l': 6,\n",
       " '##n': 10,\n",
       " 'a': 5,\n",
       " '##ed': 23,\n",
       " '##l': 2,\n",
       " 'd': 15,\n",
       " 'r': 14,\n",
       " 'n': 17,\n",
       " 'lo': 19,\n",
       " '##o': 9,\n",
       " 'long': 21,\n",
       " '##lk': 25,\n",
       " '##r': 7,\n",
       " '##a': 1,\n",
       " '##e': 4,\n",
       " '##d': 8,\n",
       " 'k': 12,\n",
       " '##er': 22}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_trainer.train_tokenizer(\n",
    "    training_data, real_wordpiece_tokenizer\n",
    ")\n",
    "real_wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850a280-fa70-4c6c-8419-71ecba0e5816",
   "metadata": {},
   "source": [
    "## Tokenized Output for WordPiece\n",
    "\n",
    "WordPiece uses subword tokens to represent words while reducing unknown tokens. The prefix ```##``` indicates a subword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed2a131-4f63-46e3-a96d-be8dd308331b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', '##er', 'walk', '##ed', 'a', 'long', 'walk']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bc8c0af-4786-46f1-ae9f-37f15f784b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', '##lk']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce4ce5-c263-42c2-ae35-c3ec10456573",
   "metadata": {},
   "source": [
    "## Handling Unknown Tokens in WordPiece and HuggingFace WordPiece\n",
    "\n",
    "This tokenizer splits unknown words into subword components but might fail without an ```[UNK]``` token. The implementationwith HuggingFace WordPiece adds special tokens like ```[UNK]``` to handle out-of-vocabulary words more gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a0766ba-2c25-4ba5-bee8-68b689d20de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "WordPiece error: Missing [UNK] token from the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mreal_wordpiece_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshe walked\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[1;31mException\u001b[0m: WordPiece error: Missing [UNK] token from the vocabulary"
     ]
    }
   ],
   "source": [
    "real_wordpiece_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f3bce15-87a6-45f9-9c68-13d5276bb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HuggingFace WordPiece and special tokens\n",
    "\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "unk_token = \"[UNK]\"\n",
    "\n",
    "wordpiece_model = WordPiece(unk_token=unk_token)\n",
    "wordpiece_tokenizer = Tokenizer(wordpiece_model)\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "wordpiece_trainer = WordPieceTrainer(\n",
    "    vocab_size=28,\n",
    "    special_tokens=[unk_token]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "286938df-c113-454a-8eb0-f31b62809d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 3,\n",
       " 'o': 8,\n",
       " 'd': 2,\n",
       " '##l': 12,\n",
       " '##n': 18,\n",
       " 'n': 7,\n",
       " '##lk': 21,\n",
       " 'l': 6,\n",
       " '##ng': 25,\n",
       " 'a': 1,\n",
       " '##g': 19,\n",
       " '##o': 17,\n",
       " 'w': 10,\n",
       " 'walke': 23,\n",
       " 'r': 9,\n",
       " '##r': 15,\n",
       " 'walked': 27,\n",
       " '##a': 11,\n",
       " 'lo': 24,\n",
       " '##k': 13,\n",
       " 'walk': 22,\n",
       " 'g': 4,\n",
       " 'walker': 26,\n",
       " '##e': 14,\n",
       " '##d': 16,\n",
       " '[UNK]': 0,\n",
       " 'k': 5,\n",
       " 'wa': 20}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.train_from_iterator(\n",
    "    training_data, \n",
    "    wordpiece_trainer\n",
    ")\n",
    "wordpiece_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d6029ca-d287-478e-a8f3-e60b67341427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walker', 'walked', 'a', 'lo', '##ng', 'walk']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cc56146-dc85-410a-88b2-89098426a2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', '##lk']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecb134cf-1998-4156-8108-d8da63453ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'walked']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e606095-0163-4335-93c6-515ebb03a608",
   "metadata": {},
   "source": [
    "## Unigram Tokenization\n",
    "\n",
    "Unigram tokenization selects the smallest set of subwords that maximize the likelihood of the training data, balancing efficiency and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "550cb839-15b7-4faa-b9f1-d4306499c570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walke': 1,\n",
       " 'e': 7,\n",
       " 'n': 11,\n",
       " 'o': 9,\n",
       " 'd': 8,\n",
       " 'k': 2,\n",
       " 'l': 5,\n",
       " 'g': 12,\n",
       " '[UNK]': 0,\n",
       " 'walk': 4,\n",
       " 'a': 6,\n",
       " 'w': 3,\n",
       " 'r': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Unigram\n",
    "\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.models import Unigram\n",
    "\n",
    "unigram_tokenizer = Tokenizer(Unigram())\n",
    "unigram_tokenizer.pre_tokenizer = Whitespace()\n",
    "unigram_trainer = UnigramTrainer(\n",
    "    vocab_size=14, \n",
    "    special_tokens=[unk_token],\n",
    "    unk_token=unk_token,\n",
    ")\n",
    "\n",
    "unigram_tokenizer.train_from_iterator(training_data, unigram_trainer)\n",
    "unigram_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "590b324b-5daf-40d1-ae6a-899819365f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walke', 'r', 'walke', 'd', 'a', 'l', 'o', 'n', 'g', 'walk']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"walker walked a long walk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c93853bd-1812-4019-b1c4-67494f809956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'l', 'k']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"wlk\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41167cfc-76c9-458d-ab61-8922672cf436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sh', 'e', 'walke', 'd']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"she walked\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1c83e75-9391-48b6-80b8-d27cbc3901fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 1, 8]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.encode(\"she walked\").ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f216ff-d944-4ab7-953e-7d51d25e01c4",
   "metadata": {},
   "source": [
    "This notebook demonstrated four tokenization techniques: Byte Pair Encoding (BPE), WordPiece, HuggingFace WordPiece with special tokens, and Unigram tokenization. Each has its strengths and weaknesses, making them suitable for different NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef5d42-3258-4903-a446-305017133aa8",
   "metadata": {},
   "source": [
    "**Well Done!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9473c4f-e156-4cb9-922e-e4b3998a6e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
